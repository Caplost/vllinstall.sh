docker 启动命令
docker run --shm-size 500g     --network=host     --name=dpskv3     --privileged     --device=/dev/kfd     --device=/dev/dri     --group-add video     --cap-add=SYS_PTRACE     --security-opt seccomp=unconfined     -v /data:/home/     -v /opt/hyhal:/opt/hyhal:ro     -it image.sourcefind.cn:5000/dcu/admin/base/custom:vllm0.7.2-ubuntu22.04-dtk25.04-rc7-das1.4-py3.10-20250317 bash



容器内设置

export ALLREDUCE_STREAM_WITH_COMPUTE=1
export VLLM_HOST_IP=172.16.254.109
export NCCL_SOCKET_IFNAME=enp145s0f0
export GLOO_SOCKET_IFNAME=enp145s0f0
export HIP_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
export NCCL_ALGO=Ring
export VLLM_MLA_DISABLE=1


~/.bashrc


设置 ray
ray start --head --node-ip-address=172.16.254.109 --port=6379 --num-gpus=8 --num-cpus=32


ray start --address='172.16.254.109:6379' --num-gpus=8 --num-cpus=32


模型地址：https://hf-mirror.com/meituan/DeepSeek-R1-Block-INT8/blob/main/config.json


vllm serve /home/models/DeepSeek-R1-Block-INT8 --trust-remote-code --distributed-executor-backend ray --dtype bfloat16 --max-model-len 16384 -tp 16 --gpu-memory-utilization 0.9 --max-num-seqs 128 --num-speculative-tokens 1 --speculative-disable-by-batch-size 33


vllm serve /home/models/DeepSeek-R1-Block-INT8 \
  --trust-remote-code \
  --distributed-executor-backend ray \
  --dtype bfloat16 \
  --max-model-len 16384 \
  -tp 16 \
  --gpu-memory-utilization 0.85 \
  --max-num-seqs 32 \
  --num-speculative-tokens 1 \
  --speculative-disable-by-batch-size 33 &



  vllm serve /home/models/DeepSeek-R1-Block-INT8 \
  --trust-remote-code \
  --distributed-executor-backend ray \
  --dtype bfloat16 \
  --max-model-len 16384 \
  -tp 16 \
  --gpu-memory-utilization 0.85 \
  --max-num-seqs 32 \
  --enable-prefix-caching \
  --enable-chunked-prefill \
  --swap-space 16 
  &


#!/bin/bash
source env_setup.sh

# 模型路径 - 使用美团的INT8量化模型
MODEL_PATH="meituan/DeepSeek-R1-Block-INT8"
# 或使用本地模型路径
# MODEL_PATH="/path/to/local/model"

# VLLM Serve命令
vllm serve $MODEL_PATH \
    --tensor-parallel-size 8 \
    --pipeline-parallel-size 2 \
    --block-size 64 \
    --swap-space 16 \
    --gpu-memory-utilization 0.9 \
    --max-model-len 8192 \
    --max-num-seqs 64 \
    --max-num-batched-tokens 32768 \
    --enable-prefix-caching \
    --enable-chunked-prefill \
    --worker-use-ray \
    --trust-remote-code \
    --quantization int8 \
    --host 0.0.0.0 \
    --port 8000

huggingface-cli download meituan/DeepSeek-R1-Block-INT8 --local-dir /data/models/DeepSeek-R1-Block-INT8 &


curl -X POST http://153.37.96.42:21006/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "/home/models/DeepSeek-R1-Block-INT8",
    "messages": [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "Hello, can you help me with a task?"}
    ],
    "temperature": 0.7,
    "max_tokens": 100,
    "stream": false
  }'