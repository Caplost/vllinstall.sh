# vllm+Ray 多机多卡部署指南

本指南提供了使用Docker方式部署vllm+Ray的完整步骤，支持海光GPU多机多卡环境。

## 1. 部署前准备

### 系统要求
- 操作系统: Ubuntu 22.04 / Rocky Linux
- CUDA/ROCm: 已安装海光GPU驱动
- Docker: 已安装并配置
- 网络: 所有节点间可互相访问

### 文件说明
- `config.env` - 配置文件，包含所有可自定义项
- `deploy_vllm.sh` - 主部署脚本
- 自动生成的脚本:
  - `download_model.sh` - 模型下载脚本
  - `start_service.sh` - 服务启动脚本
  - `check_status.sh` - 状态检查脚本

## 2. 部署步骤

### 2.1 配置环境

1. 创建工作目录并下载部署文件:
```bash
mkdir -p vllm-deploy
cd vllm-deploy
wget -O deploy_vllm.sh https://your-script-location/deploy_vllm.sh
wget -O config.env https://your-script-location/config.env
chmod +x deploy_vllm.sh
```

2. 编辑配置文件(config.env)，设置:
   - 项目路径 (`PROJECT_PATH`)
   - 模型存储路径 (`MODEL_PATH`)
   - 其他参数按需调整
   
### 2.2 部署头节点

在头节点上执行:
```bash
./deploy_vllm.sh head <头节点IP>
```

这将:
- 拉取基础Docker镜像
- 创建安装脚本
- 启动Ray头节点容器
- 生成便捷管理脚本

### 2.3 部署工作节点

在每个工作节点上执行:
```bash
./deploy_vllm.sh worker <头节点IP> <当前工作节点IP>
```

这将:
- 拉取基础Docker镜像
- 创建安装脚本
- 启动Ray工作节点容器并连接到头节点

### 2.4 验证部署

在头节点上执行:
```bash
./check_status.sh
```

或使用命令:
```bash
./deploy_vllm.sh status
```

## 3. 模型管理

### 3.1 下载模型

使用便捷脚本下载模型:
```bash
./download_model.sh meta-llama/Llama-3-8B-instruct huggingface
```

或使用命令:
```bash
./deploy_vllm.sh model-download meta-llama/Llama-3-8B-instruct huggingface
```

### 3.2 同步模型到工作节点

1. 编辑工作节点列表文件worker_nodes.txt，添加工作节点IP地址
2. 下载模型时系统会询问是否同步到工作节点

也可在模型下载后手动同步:
```bash
docker exec vllm-ray-head python /model_manager.py sync Llama-3-8B-instruct --nodes /home/worker_nodes.txt
```

### 3.3 查看已下载模型

```bash
./deploy_vllm.sh status
```

或直接执行:
```bash
docker exec vllm-ray-head python /model_manager.py list
```

## 4. 启动推理服务

### 4.1 使用便捷脚本

```bash
./start_service.sh Llama-3-8B-instruct 8
```

参数说明:
- 第一个参数: 模型名称(与下载时的基础名称相同)
- 第二个参数: tensor_parallel_size(使用的GPU数量，0表示自动使用全部可用GPU)

### 4.2 使用命令行

```bash
./deploy_vllm.sh start-service Llama-3-8B-instruct 8
```

### 4.3 访问服务

服务启动后可通过 `http://<头节点IP>:8000` 访问，支持以下API端点:
- `/` - 状态检查
- `/generate` - 文本生成
- `/batch_generate` - 批量文本生成
- `/metrics` - 性能指标
- `/health` - 健康检查

示例请求:
```bash
curl -X POST http://<头节点IP>:8000/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Hello, I am a", "max_tokens": 50}'
```

## 5. 常见问题

### 5.1 GPU设备未检测到

检查海光GPU驱动是否正确安装:
```bash
ls /dev/dri/renderD*
```

### 5.2 Ray集群连接问题

确保:
- 所有节点网络互通
- 防火墙允许Ray端口(默认6379)
- 节点IP地址正确

### 5.3 模型加载问题

检查:
- 模型是否完整下载
- 是否有足够的GPU内存
- 日志是否有错误信息:
```bash
docker logs vllm-ray-head
```

## 6. 高级配置

### 6.1 自定义模型参数

编辑config.env中的配置:
- DEFAULT_TP_SIZE - 默认Tensor并行大小
- DEFAULT_BATCH_SIZE - 批处理大小
- QUANTIZATION - 量化配置(none, int8, int4)

### 6.2 调整资源利用

在Ray集群已启动的情况下，可以通过设置不同的tensor_parallel_size来调整模型如何利用GPU资源:
- 较小值(如2或4): 适合同时加载多个模型
- 较大值(如8或16): 适合加载大型模型

### 6.3 自定义API服务

修改config.env中的API_PORT以更改服务端口。

## 7. 性能优化建议

- 增加共享内存大小(SHM_SIZE)可提高大批量请求的稳定性
- 对于高吞吐量场景，增加批处理大小
- 使用更大的tensor_parallel_size可加快单个请求的响应速度
- 确保节点间使用高速网络连接，特别是当tensor_parallel_size跨节点时