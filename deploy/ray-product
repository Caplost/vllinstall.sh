#!/usr/bin/env python3
import ray
from ray import serve
from vllm import LLM, SamplingParams
from fastapi import FastAPI, Request
import time
import os

# 初始化Ray和Serve，确保所有节点都被识别
# 如果是多机集群，确保已经通过ray start将所有节点加入
ray.init(address="auto", namespace="vllm")

# 检查集群资源
print("集群资源情况:")
print(ray.cluster_resources())

serve.start(detached=True, http_options={"host": "0.0.0.0", "port": 8000})

# 使用更敏感的自动扩展配置
@serve.deployment(
    ray_actor_options={"num_gpus": 8},
    autoscaling_config={
        "min_replicas": 1, 
        "max_replicas": 16,
        "target_num_ongoing_requests_per_replica": 10,  # 降低每副本请求数阈值
        "upscale_delay_s": 10,  # 减少扩展延迟
        "downscale_delay_s": 300,
    },
    max_ongoing_requests=100
)
class VLLMDeployment:
    def __init__(self):
        # 打印节点信息，便于调试
        print(f"Initializing deployment on node: {ray.get_runtime_context().get_node_id()}")
        
        self.llm = LLM(
            model="/home/models/DeepSeek-R1-Distill-Qwen-32B",
            tensor_parallel_size=8,
            trust_remote_code=True,
            max_model_len=131072
        )
    
    async def __call__(self, request: Request):
        # 记录请求开始时间，方便观察处理时间
        start_time = time.time()
        
        data = await request.json()
        prompt = data.get("prompt", "")
        max_tokens = data.get("max_tokens", 1000)
        
        sampling_params = SamplingParams(
            max_tokens=max_tokens,
            temperature=data.get("temperature", 0.7)
        )
        
        outputs = self.llm.generate([prompt], sampling_params)
        completion_text = outputs[0].outputs[0].text
        
        # 记录处理时间
        process_time = time.time() - start_time
        print(f"请求处理时间: {process_time:.2f}秒, 节点: {ray.get_runtime_context().get_node_id()}")
        
        return {
            "id": f"cmpl-{time.time()}",
            "object": "text_completion",
            "created": int(time.time()),
            "model": data.get("model", "DeepSeek-R1"),
            "choices": [{"text": completion_text, "index": 0, "finish_reason": "stop"}],
            "usage": {
                "prompt_tokens": len(outputs[0].prompt_token_ids),
                "completion_tokens": len(outputs[0].outputs[0].token_ids),
                "total_tokens": len(outputs[0].prompt_token_ids) + len(outputs[0].outputs[0].token_ids)
            }
        }

# 使用正确的部署方式
deployment = VLLMDeployment.bind()
handle = serve.run(deployment, name="vllm_service")

print("服务已启动，可以通过 http://localhost:8000/ 访问")
print("监控副本数: ray status -v")